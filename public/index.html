<!doctype html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Do Explanations Generalize across Large Reasoning Models?</title>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="Exploring whether reasoning chains can serve as stable, interpretable guides that lead different AI systems to consistent answers.">
    <meta property="og:title" content="Do Explanations Generalize across Large Reasoning Models?">
    <meta property="og:url" content="https://genex.baulab.info/">
    <meta property="og:image" content="https://genex.baulab.info/images/fv-thumb.png">
    <meta property="og:description" content="Do explanations generalize across large reasoning models?">
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Do Explanations Generalize across Large Reasoning Models?">
    <meta name="twitter:description" content="Do explanations generalize across large reasoning models?">
    <meta name="twitter:image" content="https://genex.baulab.info/images/fv-thumb.png">
    
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <style>
        .relatedthumb {
            float: left;
            width: 200px;
            margin: 3px 10px 7px 0;
        }

        .relatedblock {
            clear: both;
            display: inline-block;
        }

        .bold-sc {
            font-variant: small-caps;
            font-weight: bold;
        }

        .cite,
        .citegroup {
            margin-bottom: 8px;
        }

        :target {
            background-color: yellow;
        }

        figure {
            margin: 2em 0;
        }

        figure img {
            max-width: 100%;
            height: auto;
        }

        figcaption {
            margin-top: 1em;
            font-size: 0.95em;
            line-height: 1.5;
        }

        .citation {
            margin: 1.5em 0;
            padding: 1em;
            background: #f8f8f8;
            border-left: 3px solid #333;
        }

        .citation img {
            max-width: 200px;
            max-height: 150px;
            float: left;
            margin-right: 1em;
            margin-bottom: 0.5em;
        }

        .citation::after {
            content: "";
            display: table;
            clear: both;
        }
    </style>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date()); 
        gtag('config', 'G-FD12LWN557');
    </script>
</head>

<body class="nd-docs">
    <div class="nd-pageheader">
        <div class="container">
            <h1 class="lead">
                <span class="widenobr">Do Explanations Generalize across Large Reasoning Models?</span>
            </h1>
            <address>
                <span><a href="https://koyenapal.github.io/" target="_blank">Koyena Pal*</a><sup>1</sup>,</span>
                <span><a href="https://baulab.info/" target="_blank">David Bau</a><sup>1</sup>,</span>
                <span><a href="https://csinva.io/" target="_blank">Chandan Singh</a><sup>2</sup></span><br>
                <span><sup>1</sup><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern University</a>,</span>
                <span><sup>2</sup><a href="https://www.microsoft.com/en-us/research/" target="_blank">Microsoft Research</a></span><br>
                <span><sup>*</sup>Work done as part of <a href="https://www.cbai.ai/" target="_blank">CBAI Fellowship</a></span>
            </address>
        </div>
    </div>

    <div class="container">
        <div class="row justify-content-center text-center">
            <div class="col-12">
                <p>
                    <a href="https://arxiv.org/abs/2601.11517" class="d-inline-block p-3 align-top" target="_blank">
                        <img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;" alt="ArXiv Preprint thumbnail">
                        <br>Preprint<br>ArXiv
                    </a>
                    <a href="https://github.com/KoyenaPal/genex/" class="d-inline-block p-3 align-top" target="_blank">
                        <img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Github code thumbnail">
                        <br>Source Code<br>Github
                    </a>
                </p>

                <div class="card" style="max-width: 1020px; margin: 0 auto;">
                    <div class="card-block">
                        <h3>Generalization of Chain of Thoughts (CoTs)</h3>
                        <p>
                            While models and humans naturally explain concepts differently, we explore something more fundamental: <strong>can reasoning chains serve as stable, interpretable guides that lead different AI systems to consistent answers?</strong>
                            A good explanation isn't just correct—it should be learnable. If we give an explanation to another agent, they should understand it and draw the intended conclusions. We quantify this through cross-model CoT generalization.
                        </p>
                        <p>
                            We find that explanations <em>can</em> generalize across models, though effectiveness varies significantly by which models are involved. Ensembled explanations often improve consistency, and more consistent explanations correlate with higher human preference.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col">
                <h2>Approach</h2>

                <p>We test four methods for eliciting and using reasoning explanations across models:</p>

                <div style="margin: 1.5em 0;">
                    <p><strong>(A) Empty CoT:</strong> No reasoning provided (baseline)</p>
                    <p><strong>(B1) Default CoT:</strong> Model uses its own reasoning with deterministic decoding</p>
                    <p><strong>(B2) Sampled CoT:</strong> Model uses its own reasoning with nucleus sampling</p>
                    <p><strong>(C) Transfer CoT:</strong> One model's reasoning is transferred to another model</p>
                    <p><strong>(D) Ensemble CoT:</strong> Multiple models generate candidate sentences, and an evaluator selects the least surprising option at each step. The selected sentence is added to context and the process repeats until thinking end tag is generated.</p>
                </div>

                <p>For a generator model l<sub>gen</sub> and problem x, we elicit a reasoning explanation z = l<sub>gen</sub>(x) within thinking tags before the model produces an answer. We then test whether this explanation generalizes by giving it to a different evaluator model l<sub>eval</sub> and measuring if it reaches the same conclusion. To isolate the effect of reasoning rather than answer copying, we remove explicit answer declarations from explanations using an LLM filter.</p>

                <figure class="text-center">
                    <img src="images/Paper/overview.png" alt="Overview of CoT generalization methods" class="bigfig">
                    <figcaption>
                        <strong>Methods for eliciting reasoning chains and exploring generalization of Chain-of-Thought (CoT).</strong> 
                        The figure illustrates four approaches to modifying or replacing model-generated CoTs. Panel (E) (bottom-right) shows 
                        how CoT generalization is evaluated for the MedCalc-Bench dataset across a set of models (A–E), where each model's reasoning 
                        can be substituted with one of the following variations: <strong>(A) Empty CoT:</strong> No reasoning text is provided between the 
                        model's thinking tags. <strong>(B) Default CoT:</strong> The model's own reasoning is used. (2.1) uses deterministic decoding, while (2.2) 
                        uses nucleus sampling (do_sampling=True). <strong>(C) Transfer CoT:</strong> Reasoning from one model is directly transferred to another, 
                        replacing its own. <strong>(D) Ensembled CoT:</strong> A generator–evaluator loop. Generator models produce <em>n</em> = 3 candidate sentences 
                        (≤ 15 tokens each), forming <em>k</em> candidates. These are scored by the evaluator, and the least surprising candidate (lowest perplexity) 
                        is appended to the growing ensembled thought. This updated context is fed back into the generators, and the process repeats until an 
                        end-of-thought or maximum token limit is reached.
                    </figcaption>
                </figure>

                <h3 style="margin-top: 2em;">Evaluation Setup</h3>
                
                <p>We evaluate these methods across <strong>5 large reasoning models</strong> — NRR (1.5B), OpenT (7B), OSS (20B), QwQ (32B), and DAPO (32B) — on two benchmarks:</p>
                
                <ul style="margin-left: 1.5em;">
                    <li><strong>MedCalc-Bench:</strong> 100 medical calculation tasks requiring domain-specific reasoning</li>
                    <li><strong>Instruction Induction:</strong> 100 samples across 20 general reasoning tasks (extended with 12 new tasks)</li>
                </ul>

                <p style="margin-top: 1.5em;">We measure generalization of CoT through <strong>consistency</strong> (do models reach the same conclusion?) and track its effect on <strong>accuracy</strong> (is that conclusion correct?).</p>
            </div>
        </div>

        <div class="row">
            <div class="col">
                <h2>Matching Model Responses</h2>

                <figure class="text-center">
                    <img src="images/Paper/consistency.png" alt="Consistency analysis across methods" class="bigfig">
                    <figcaption>
                        Average pairwise consistency across thought settings in MedCalc-Bench (above) and
                        Instruction Induction (below). For thought variations indicating Ensemble CoT, models listed before
                        the slash (/) serve as generators, while the model after the slash acts as the judge/evaluator. Results
                        are reported both for the full text and for text with the final answer removed.
                    </figcaption>
                </figure>

                <p>The figure above shows average pairwise consistency across different CoT methods. Transfer and ensemble approaches substantially increase consistency compared to default or empty baselines.</p>

                <p><strong>Key findings:</strong></p>
                <ul style="margin-left: 1.5em;">
                    <li><strong>Ensemble methods boost consistency:</strong> Using multiple models to generate and evaluate candidates achieves higher consistency than single-model approaches</li>
                    <li><strong>Model choice matters:</strong> The effectiveness of transfer and ensemble methods varies significantly depending on which models serve as generators versus evaluators</li>
                    <li><strong>Pattern holds across benchmarks:</strong> Both MedCalc-Bench and Instruction Induction show similar trends, with ensemble and transfer methods outperforming baselines</li>
                </ul>

                <p>These results suggest that <strong>models whose explanations transfer well to others also tend to be effective generators in ensemble settings.</strong></p>

                <h3 style="margin-top: 3em;">Impact on Accuracy</h3>

                <figure class="text-center">
                    <img src="images/Paper/transfer_effect.png" alt="CoT transfer effect analysis" class="bigfig">
                    <figcaption>
                        <strong>CoT Transfer Effect Analysis (MedCalc-Bench).</strong> Distribution of outcomes when CoT reasoning is transferred across models, comparing predictions with CoT (answers removed) versus without CoT (empty baseline). Each CoT setting shows five outcomes: <span style="color: green;">Wrong→Correct</span> (CoT successfully corrects errors), <span style="color: red;">Correct→Wrong</span> (CoT misleads from correct to incorrect), <span style="color: blue;">Correct→Correct</span> (CoT maintains correct predictions), Wrong→Wrong(Match) (both incorrect with same answer), and Wrong→Wrong(Diff) (both incorrect with different answers). Settings are sorted by Wrong→Correct rate (descending).
                    </figcaption>
                </figure>

                <p>While CoT increases consistency, its effect on accuracy varies. Some CoT methods successfully correct errors (Wrong→Correct), while others can mislead models from correct to incorrect predictions (Correct→Wrong). The balance between these outcomes depends on both the CoT method and the models involved.</p>

                <p><strong>Notable patterns:</strong> Ensemble and certain transfer methods show higher rates of error correction, though no method universally improves accuracy across all model pairs. This highlights the importance of considering both consistency and accuracy when evaluating CoT generalization.</p>
       
                <h3 style="margin-top: 3em;">Human Preference Study</h3>

                <figure class="text-center">
                    <img src="images/Paper/user_study.png" alt="Human preference correlation analysis" class="bigfig">
                    <figcaption>
                        <strong>Consistency correlates more strongly with human preference than accuracy.</strong> Scatter plots show relationships between model consistency (left) and accuracy (right) with human ratings across four dimensions: Clarity of Steps, Ease of Following, Confidence, and Best Overall. Linear regression lines reveal that consistency has stronger and more separable correlations with user preferences.
                    </figcaption>
                </figure>

                <p>We conducted a user study with 15 participants (computer science and healthcare researchers) who rated CoT explanations on clarity, ease of following, and confidence. The results reveal a key finding: <strong>consistency between models is a better predictor of human preference than accuracy.</strong></p>

                <p>This suggests that explanations which generalize well across models—leading different systems to the same conclusion—are also more convincing and understandable to human users. Ensemble methods, which achieved the highest consistency, also received the best ratings across all evaluation criteria.</p>
            </div>
        </div>
                    
        <div class="row">
            <div class="col">
                <h2>Related Work</h2>
                
                <p>Our work builds upon insights in other work that has examined ways to generate, improve, assess and evaluate explanations:</p>
                
                <div class="citation">
                    <a href="https://dl.acm.org/doi/10.5555/3600270.3602070">
                        <img src="images/wei-2022.png" alt="Wei et al. 2022">
                        Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou. <em>Chain-of-thought prompting elicits reasoning in large language models.</em> 2022.
                    </a>
                    <p><strong>Notes:</strong> Introduces chain-of-thought prompting and shows that generating intermediate reasoning steps dramatically improves language model performance on complex reasoning tasks.</p>
                </div>
                
                <div class="citation">
                    <a href="https://arxiv.org/abs/2505.05410">
                        <img src="images/chen-2025.png" alt="Chen et al. 2025">
                        Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez. <em>Reasoning Models Don't Always Say What They Think</em> 2025.
                    </a>
                    <p><strong>Notes:</strong> Demonstrates that reasoning models often fail to verbalize their actual reasoning processes in CoTs, with reveal rates frequently below 20%, highlighting fundamental challenges in using CoT monitoring for AI safety.</p>
                </div>
                
                <div class="citation">
                    <a href="https://aclanthology.org/2020.acl-main.386/">
                        <img src="images/jacovi-2020.png" alt="Jacovi & Goldberg 2020">
                        Alon Jacovi, Yoav Goldberg. <em>Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?</em> 2020.
                    </a>
                    <p><strong>Notes:</strong> Distinguishes faithfulness from plausibility and argues for graded rather than binary faithfulness metrics.</p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col">
                <h2>How to Cite</h2>

                <p>This work is available on arXiv and can be cited as follows:</p>

                <div class="card">
                    <h3 class="card-header">Bibliography</h3>
                    <div class="card-block">
                        <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
                            Koyena Pal, David Bau, and Chandan Singh. "<em>Do Explanations Generalize across Large Reasoning Models?</em>" arXiv preprint arXiv:2601.11517, (2026).
                        </p>
                    </div>
                    <h3 class="card-header">BibTeX</h3>
                    <div class="card-block">
                        <pre class="card-text clickselect">@misc{pal2026explanationsgeneralizelargereasoning,
      title={Do explanations generalize across large reasoning models?}, 
      author={Koyena Pal and David Bau and Chandan Singh},
      year={2026},
      eprint={2601.11517},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2601.11517}, 
}</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <footer class="nd-pagefooter">
        <div class="row">
            <div class="col-6 col-md text-center">
                <a href="https://baulab.info/">About the Bau Lab</a>
            </div>
        </div>
    </footer>

    <script>
        $(document).on('click', '.clickselect', function (ev) {
            var range = document.createRange();
            range.selectNodeContents(this);
            var sel = window.getSelection();
            sel.removeAllRanges();
            sel.addRange(range);
        });
    </script>
</body>

</html>